{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition.truncated_svd import TruncatedSVD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score as F1, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159566  \":::::And for the second time of asking, when ...      0\n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159569  And it looks like it was actually you who put ...      0\n",
       "159570  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('toxic_comments.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20103cf0358>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.toxic.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработаем текст. Приведем текст к нижнему регистру. С помощью регулярных выражений уберем лишние символы (Оставим только буквы и цифры), а также слишком короткие слова. Затем лемматизируем наши слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "def text_preprocessing(text):\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(' \\w{1,2} ', ' ', text)\n",
    "    text = re.sub('^\\w{1,2} ', ' ', text)\n",
    "    text = re.sub(' \\w{1,2}$', ' ', text)\n",
    "    text = re.sub(' {2,}', ' ', text)\n",
    "    text = re.sub(' $', '', text)\n",
    "    text = re.sub('^ ', '', text)\n",
    "    \n",
    "    text = text.split(' ')\n",
    "    s = ''\n",
    "    for w in text:\n",
    "        w = stemmer.stem(w)\n",
    "        s += w + ' '\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'want play with friend toy 122 time '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing('i wanted to   (*)() play   with friend\\'s toy 122 times 1 r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prep_text'] = df.text.apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>prep_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explan whi the edit made under usernam hardcor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>aww match this background colour m seem stuck ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man m realli not tri edit war s just that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more can make ani real suggest improv wonder t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are hero ani chanc you rememb what pag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           prep_text  \n",
       "0  explan whi the edit made under usernam hardcor...  \n",
       "1  aww match this background colour m seem stuck ...  \n",
       "2  hey man m realli not tri edit war s just that ...  \n",
       "3  more can make ani real suggest improv wonder t...  \n",
       "4  you sir are hero ani chanc you rememb what pag...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся статистической векторизацией - Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop,\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.4,\n",
    "    min_df=10)\n",
    "vecs = vectorizer.fit_transform(df.prep_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 70161)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем слишком длинный вектор фичей. Укоротим его с помощью tSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 100\n",
    "tsvd = TruncatedSVD(DIM)\n",
    "vecs = tsvd.fit_transform(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tsvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также добавим семантическую составляющую наших комментариев. Используем Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.prep_text.apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['explan', 'whi', 'the', 'edit', 'made', 'under', 'usernam', 'hardcor', 'metallica', 'fan', 'were', 'revert', 'they', 'weren', 'vandal', 'just', 'closur', 'some', 'gas', 'after', 'vote', 'new', 'york', 'doll', 'fac', 'and', 'pleas', 'don', 'remov', 'the', 'templat', 'from', 'the', 'talk', 'page', 'sinc', 'm', 'retir', 'now', '205']),\n",
       "       list(['aww', 'match', 'this', 'background', 'colour', 'm', 'seem', 'stuck', 'with', 'thank', 'talk', '51', 'januari', '2016', 'utc']),\n",
       "       list(['hey', 'man', 'm', 'realli', 'not', 'tri', 'edit', 'war', 's', 'just', 'that', 'this', 'guy', 'constant', 'remov', 'relev', 'inform', 'and', 'talk', 'me', 'through', 'edit', 'instead', 'my', 'talk', 'page', 'seem', 'care', 'more', 'about', 'the', 'format', 'than', 'the', 'actual', 'info']),\n",
       "       ...,\n",
       "       list(['spitzer', 'umm', 'there', 'actual', 'articl', 'for', 'prostitut', 'ring', 'crunch', 'captain']),\n",
       "       list(['and', 'look', 'like', 'was', 'actual', 'you', 'who', 'put', 'the', 'speedi', 'have', 'the', 'first', 'version', 'delet', 'now', 'that', 'look', 'it']),\n",
       "       list(['and', 'realli', 'don', 'think', 'you', 'understand', 'came', 'here', 'and', 'idea', 'was', 'bad', 'right', 'away', 'what', 'kind', 'communiti', 'goe', 'you', 'have', 'bad', 'idea', 'away', 'instead', 'help', 'rewrit', 'them'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(705127952, 877291300)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.train(corpus, epochs=100, total_examples=w2v.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rude', 0.5269500613212585),\n",
       " ('uncivil', 0.5099079608917236),\n",
       " ('you', 0.49448448419570923),\n",
       " ('anyway', 0.4640296995639801),\n",
       " ('him', 0.4636613130569458),\n",
       " ('i', 0.45430469512939453),\n",
       " ('sarcast', 0.4503617286682129),\n",
       " ('just', 0.4470093846321106),\n",
       " ('here', 0.44522625207901),\n",
       " ('verbal', 0.4451678991317749)]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('me')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы векторизовать предложение - используем среднее всех его слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    vec = np.zeros(100)\n",
    "    words = tokenizer.tokenize(text)\n",
    "    num = 1\n",
    "    for w in words:\n",
    "        if w in w2v.wv:\n",
    "            vec += w2v.wv.get_vector(w)\n",
    "            num += 1\n",
    "    vec /= num\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explan whi the edit made under usernam hardcor metallica fan were revert they weren vandal just closur some gas after vote new york doll fac and pleas don remov the templat from the talk page sinc m retir now 205 '"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.prep_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.57777494e-01,  1.49769387e-01,  1.21814415e-01, -6.40941002e-01,\n",
       "        3.13056204e-01, -4.48752372e-01,  6.40842772e-01, -7.62335802e-02,\n",
       "       -3.84472043e-01,  2.16885222e-02, -4.00070243e-01,  5.89380394e-01,\n",
       "       -5.88857256e-01,  4.35982627e-01, -6.46270044e-02,  2.74916040e-02,\n",
       "        1.36308812e-01, -4.78786338e-01,  2.06232443e-01, -2.98873795e-01,\n",
       "       -1.74627282e-01, -1.55874471e-01,  2.97054067e-01, -2.16576761e-01,\n",
       "        8.32982781e-02, -2.73566370e-01,  9.41543165e-01,  4.20735305e-01,\n",
       "       -8.69705962e-01, -3.95261655e-01, -2.04809698e-01,  1.46458290e-01,\n",
       "       -1.90899984e-01, -9.87277999e-01, -1.27876188e-01, -1.49802544e-01,\n",
       "        4.82116463e-02, -6.16401264e-01,  4.18834172e-02, -2.02733190e-01,\n",
       "        5.63612245e-01, -5.81246469e-02, -2.65248425e-01, -3.67526827e-01,\n",
       "       -2.75985818e-01,  5.35434137e-01, -8.68563281e-03, -3.60626262e-02,\n",
       "       -1.69759758e-01,  1.94456302e-01, -1.63639742e-01,  8.26738201e-02,\n",
       "       -1.20101538e+00,  4.28267464e-01,  6.81324254e-02,  7.10420525e-01,\n",
       "       -6.24825391e-01,  6.30580746e-01, -1.40363029e-02, -8.36106899e-02,\n",
       "        3.18901995e-02,  1.71980660e-01,  6.14659508e-01, -6.63603478e-01,\n",
       "        2.42759563e-01, -2.57083460e-01,  3.60351463e-01, -7.10833516e-02,\n",
       "        4.44536967e-01, -8.98024536e-02, -5.78215133e-01,  5.14550773e-02,\n",
       "        1.50133250e-01,  2.61893622e-01,  2.77518173e-01,  8.98629930e-01,\n",
       "        1.69306843e-01,  4.90100309e-01, -2.35808285e-02, -1.17269239e-01,\n",
       "        8.24835364e-01, -1.80916831e-01,  6.74957771e-02, -5.75313574e-02,\n",
       "        5.79216454e-01, -3.77094144e-02,  8.72085446e-03, -1.77089189e-01,\n",
       "        5.41704215e-01,  5.28232460e-01,  6.91270871e-01, -4.87455387e-01,\n",
       "        7.49698863e-04,  4.61827730e-01, -3.16991825e-03,  7.42442365e-01,\n",
       "        2.13749779e-01,  5.70375952e-01,  2.21118826e-01, -9.42665879e-02])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec(df.prep_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что всё работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = df.prep_text.apply(text2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = vectors.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.zeros((arr.shape[0], 100))\n",
    "for i in range(len(arr)):\n",
    "    vec[i] = arr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 100)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь соединим статистическую и семантическую части наших текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del w2v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((vecs, vec), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборку на тренировочную и тестовую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = list(kf.split(X, y))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализуем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы выбрать лучшие гиперпараметры для модели проведем поиск по сетке с кроссвалидацией с помощью StratifiedKFold, т.к. мы имеем неодинаковые доли классов 0 и 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(3, shuffle=True)\n",
    "scorer = make_scorer(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=SGDClassifier(alpha=0.0001, average=False,\n",
       "                                     class_weight=None, early_stopping=False,\n",
       "                                     epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "                                     l1_ratio=0.15, learning_rate='optimal',\n",
       "                                     loss='hinge', max_iter=1000,\n",
       "                                     n_iter_no_change=5, n_jobs=None,\n",
       "                                     penalty='l2', power_t=0.5,\n",
       "                                     random_s...e, tol=0.001,\n",
       "                                     validation_fraction=0.1, verbose=0,\n",
       "                                     warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'loss': ['hinge', 'log', 'modified_huber',\n",
       "                                  'squared_hinge', 'perceptron', 'squared_loss',\n",
       "                                  'huber', 'epsilon_insensitive',\n",
       "                                  'squared_epsilon_insensitive']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber',\n",
    "    'squared_hinge', 'perceptron', 'squared_loss',\n",
    "    'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "}\n",
    "sgd_grid = GridSearchCV(SGDClassifier(), params, cv=cv, scoring=scorer)\n",
    "sgd_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.739280301234452"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sgd_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7531770686246823"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, побили бейслайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем поработать с алгоритмом градиентного бустинга из библиотеки lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "lightgbm.basic.LightGBMError: Check failed: config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f at d:\\a\\1\\s\\python-package\\compile\\src\\boosting\\rf.hpp, line 35 .\n",
      "\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "                                      colsample_bytree=1.0,\n",
       "                                      importance_type='split',\n",
       "                                      learning_rate=0.1, max_depth=-1,\n",
       "                                      min_child_samples=20,\n",
       "                                      min_child_weight=0.001,\n",
       "                                      min_split_gain=0.0, n_estimators=15,\n",
       "                                      n_jobs=-1, num_leaves=31, objective=None,\n",
       "                                      random_state=None, reg_alpha=0.0,\n",
       "                                      reg_lambda=0.0, silent=True,\n",
       "                                      subsample=1.0, subsample_for_bin=200000,\n",
       "                                      subsample_freq=0),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'boosting_type': ['gbdt', 'dart', 'goss', 'rf'],\n",
       "                         'max_depth': [3, 5, 7, -1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': ['gbdt', 'dart', 'goss', 'rf'],\n",
    "    'max_depth': [3, 5, 7, -1],\n",
    "    \n",
    "}\n",
    "gb_grid = GridSearchCV(LGBMClassifier(n_estimators=15), params, cv=cv, scoring=scorer)\n",
    "gb_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.659591093056211"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gb_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6703245022730836"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='goss', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=200, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf = LGBMClassifier(**gb_grid.best_params_, n_estimators=200)\n",
    "gb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75787728026534"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили результат чуть лучше, чем от любой стохастической оптимизации линейных моделей. Здорово"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я научился работать с текстовыми данными, очищать их, приводить в понятный алгоритмам вид, используя статистические и семантические признаки текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
