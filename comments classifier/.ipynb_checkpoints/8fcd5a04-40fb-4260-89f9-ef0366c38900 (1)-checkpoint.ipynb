{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Комментарий от ревьюера:</b> \n",
    "    \n",
    "Максим, добрый день! Меня зовут Влада. Ниже в файле ты найдешь мои комментарии: <font color='green'>зеленый цвет — «все отлично»; </font> <font color='blue'>синий — «хорошо, но можно лучше (исправлять необязательно)»; </font> <font color='red'>красный — «нужно исправить».</font> Комментарии в самом коде я отделяю знаками «###». Пожалуйста, не удаляй мои комментарии, они мне нужны при повторной проверке.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition.truncated_svd import TruncatedSVD\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score as F1, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159566  \":::::And for the second time of asking, when ...      0\n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159569  And it looks like it was actually you who put ...      0\n",
       "159570  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('toxic_comments.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12e144acb70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.toxic.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы не сбалансированы, следовательно, чтобы получить объективную оценку модели (в условиях нашей задачи) нужно будет поделить данные в таком же отношении, а также использовать F1-меру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Какие выводы? Сбалансированы ли классы?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработаем текст. Приведем текст к нижнему регистру. С помощью регулярных выражений уберем лишние символы (Оставим только буквы), а также слишком короткие слова. Затем лемматизируем наши слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "def text_preprocessing(text):\n",
    "    text = re.sub('[\\W\\d]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(' \\w{1,2} ', ' ', text)\n",
    "    text = re.sub('^\\w{1,2} ', ' ', text)\n",
    "    text = re.sub(' \\w{1,2}$', ' ', text)\n",
    "    text = re.sub(' {2,}', ' ', text)\n",
    "    text = re.sub(' $', '', text)\n",
    "    text = re.sub('^ ', '', text)\n",
    "    \n",
    "    text = text.split(' ')\n",
    "    s = ''\n",
    "    for w in text:\n",
    "        w = stemmer.stem(w)\n",
    "        s += w + ' '\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "От цифр тоже, в принципе, можно было избавиться.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'want play with friend toy time '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing('i wanted to   (*)() play   with friend\\'s toy 122 times 1 r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prep_text'] = df.text.apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>prep_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explan whi the edit made under usernam hardcor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>aww match this background colour m seem stuck ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man m realli not tri edit war s just that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more can make ani real suggest improv wonder t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are hero ani chanc you rememb what pag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           prep_text  \n",
       "0  explan whi the edit made under usernam hardcor...  \n",
       "1  aww match this background colour m seem stuck ...  \n",
       "2  hey man m realli not tri edit war s just that ...  \n",
       "3  more can make ani real suggest improv wonder t...  \n",
       "4  you sir are hero ani chanc you rememb what pag...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Прекрасно, провели очистку от ненужных символов и стемминг.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поделим выборку на обучающую и тестовую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = train_test_split(df.prep_text.values, df.toxic.values, \\\n",
    "                                                          stratify=df.toxic.values, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся статистической векторизацией - Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop,\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.4,\n",
    "    min_df=10)\n",
    "vecs = vectorizer.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Словарь TF-IDF следует настраивать (fit) только по обучающей выборке. Тестовая выборка нужна для оценивания качества построенной модели, настраивать по ней параметры некорректно, для нее следует использовать метод transform.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Молодец, что не включаешь редкие и слишком частые слова в словарь (задаешь min_df и max_df).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119678, 51308)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем слишком длинный вектор фичей. Укоротим его с помощью tSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 100\n",
    "tsvd = TruncatedSVD(DIM)\n",
    "vecs = tsvd.fit_transform(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Здорово, сократили размерность вектора признаков. TruncatedSVD также нужно настраивать по обучающей выборке.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также добавим семантическую составляющую наших комментариев. Используем Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array([tokenizer.tokenize(x) for x in text_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['renata', 'you', 'on', 'ani', 'russian', 'websit', 'you', 'will', 'find', 'that', 'all', 'work', 'publish', 'even', 'after', 'are', 'display', 'and', 'not', 'a', 'thousand', 'year', 'will', 'anybodi', 'tri', 'persecut', 'anyon', 'moreov', 'the', 'way', 'soviet', 'mental', 'was', 'that', 'everyth', 'common', 'publish', 'work', 'the', 'soviet', 'union', 'could', 'have', 'been', 'use', 'for', 'ani', 'purpos', 'that', 'the', 'way', 'is', 'now', 'russia', 'the', 'there', 'a', 'claus', 'the', 'defend', 'assum', 'innoc', 'until', 'proven', 'guilti', 'that', 'case', 'until', 'wiki', 'actual', 'accus', 'an', 'extern', 'russian', 'author', 'which', 'less', 'like', 'happen', 'than', 'the', 'moon', 'fall', 'from', 'the', 'sky', 'is', 'wrong', 'just', 'have', 'some', 'vigilant', 'make', 'rule', 'imag', 'that', 'doe', 'not', 'even', 'use', 'himself']),\n",
       "       list(['luke', 'the', 'appropri', 'merger', 'was', 'the', 'previous', 'content', 'that', 'thf', 'put', 'when', 'the', 'merger', 'was', 'suggest', 'there', 'a', 'need', 'for', 'separ', 'page', 'precis', 'becaus', 'the', 'issu', 'are', 'veri', 'differ', 'and', 'the', 'tort', 'system', 'veri', 'differ', 'mike', 'huckabe', 'was', 'talk', 'about', 'health', 'care', 'cost', 'one', 'which', 'pay', 'out', 'larg', 'sum', 'money', 'for', 'medic', 'neglig', 'claim', 'his', 'view']),\n",
       "       list(['support', 'per', 'the', 'categori', 'name', 'last', 'year', 'support', 'renam', 'list', 'individu', 'dog', 'talk']),\n",
       "       ...,\n",
       "       list(['again', 'this', 'replac', 'argument', 'simpli', 'doe', 'not', 'work', 'are', 'not', 'look', 'for', 'anoth', 'imag', 'the', 'flag', 'be', 'place', 'the', 'roof', 'are', 'look', 'for', 'anoth', 'free', 'imag', 'the', 'battl', 'the', 'argument', 'about', 'panoram', 'view', 'potenti', 'better', 'argument', 'howev', 'whi', 'panoram', 'view', 'requir', 'not', 'clear', 'have', 'plenti', 'imag', 'destruct', 'we', 'realli', 'need', 'go', 'the', 'extent', 'use', 'non', 'free', 'imag', 'show', 'panorama', 'cours', 'not', 'the', 'destruct', 'could', 'easili', 'shown', 'other', 'way', 'do', 'not', 'use', 'non', 'free', 'imag', 'becaus', 'is', 'better', 'than', 'other', 'avail', 'imag', 'use', 'non', 'free', 'imag', 'becaus', 'show', 'someth', 'that', 'could', 'not', 'shown', 'ani', 'other', 'way', 'that', 'urgent', 'need', 'be', 'shown']),\n",
       "       list(['guettarda', 'revis', 'last', 'post', 'you', 'wrote', 'duncharri', 'also', 'admit', 'have', 'been', 'polit', 'him', 'you', 'can', 'to', 'his', 'userpag', 'and', 'see', 'for', 'yourself']),\n",
       "       list(['the', 'improv', 'obvious', 've', 'cite', 'the', 'major', 'literatur', 'you', 'can', 'discov', 'that', 'is', 'the', 'scholar', 'norm', 'religi', 'studi', 'check', 'the', 'various', 'world', 'class', 'peer', 'review', 'journal', 've', 'alreadi', 'cite', 'along', 'with', 'the', 'major', 'publish', 'is', 'the', 'neutral', 'form', 'and', 'is', 'error', 'free', 'the', 'onli', 'complaint', 'that', 'has', 'been', 'grub', 'is', 'that', 'is', 'not', 'the', 'common', 'form', 'henc', 'for', 'those', 'who', 'would', 'have', 'difficulti', 'the', 'bce', 'usag', 'was', 'link', 'the', 'common', 'era', 'page', 'three', 'reason', 'for', 'the', 'improv', 'the', 'onli', 'drawback', 'dealt', 'with', 'iar', 'obvious', 'cancel', 'the', 'fail', 'altern', 'control'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(518721699, 649263200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.train(corpus, epochs=100, total_examples=w2v.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rude', 0.5066874027252197),\n",
       " ('you', 0.4779347777366638),\n",
       " ('uncivil', 0.472068190574646),\n",
       " ('anyway', 0.46208927035331726),\n",
       " ('i', 0.4430062174797058),\n",
       " ('incivil', 0.4381691515445709),\n",
       " ('realli', 0.4315147399902344),\n",
       " ('just', 0.43092501163482666),\n",
       " ('here', 0.42876964807510376),\n",
       " ('person', 0.4264611303806305)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы векторизовать предложение - используем среднее всех его слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    vec = np.zeros(100)\n",
    "    words = tokenizer.tokenize(text)\n",
    "    num = 1\n",
    "    for w in words:\n",
    "        if w in w2v.wv:\n",
    "            vec += w2v.wv.get_vector(w)\n",
    "            num += 1\n",
    "    vec /= num\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'renata you on ani russian websit you will find that all work publish even after are display and not a thousand year will anybodi tri persecut anyon moreov the way soviet mental was that everyth common publish work the soviet union could have been use for ani purpos that the way is now russia the there a claus the defend assum innoc until proven guilti that case until wiki actual accus an extern russian author which less like happen than the moon fall from the sky is wrong just have some vigilant make rule imag that doe not even use himself '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23753111,  0.49222024, -0.02657572,  0.14021729,  0.17932099,\n",
       "        0.21674338, -0.39896278,  0.67697264, -0.17238194, -0.1207415 ,\n",
       "        0.04768567, -0.15381316,  0.33040389, -0.43522734,  0.0699736 ,\n",
       "       -0.10801034,  0.09849715,  0.0678515 ,  0.28427973, -0.51309259,\n",
       "        0.47689641, -0.33314044, -0.04124079, -0.19518434,  0.1257944 ,\n",
       "       -0.1353395 ,  0.29155704,  0.64987678,  0.2588062 , -0.61137042,\n",
       "       -0.09826422, -0.31583212,  0.31626497, -0.1312537 , -0.60212921,\n",
       "        0.71509158, -0.76712888,  0.11573717,  0.35562063,  0.41519024,\n",
       "        0.11252566, -0.58363425, -0.21539054,  0.54996145, -0.3784544 ,\n",
       "       -0.58232749,  0.43410188, -0.26341485,  0.41305755, -0.28113424,\n",
       "       -0.33684351,  0.44734387, -0.06690684, -0.00652272, -0.95234634,\n",
       "        0.04723173, -0.16969869, -0.52872897, -0.39501757, -0.65265724,\n",
       "        0.24765149, -0.15134728,  0.67647049, -0.2046247 ,  0.14619984,\n",
       "       -0.23914173,  0.15186642,  0.49628638,  0.47508127,  0.33516142,\n",
       "        0.65218095, -0.12063766,  0.33188417, -0.34160369, -0.04772497,\n",
       "       -0.3051435 , -0.34029266,  0.32462092,  0.15371473,  0.02181822,\n",
       "       -0.20033604,  0.70613798,  0.01840426,  0.04052735,  0.85116125,\n",
       "        0.13664991, -0.11863066,  0.84691117, -0.19142761, -0.43018051,\n",
       "        0.03181501, -1.07336429,  0.03812454, -0.35159833,  0.30640059,\n",
       "        0.00609501, -0.03531427, -0.29007996, -0.62446785,  0.26371544])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vec(text_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что всё работает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Прекрасно!\n",
    "    \n",
    "Замечу, что обучать Word2Vec следует также только по обучающей выборке.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [text2vec(x) for x in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.zeros((len(vectors), 100))\n",
    "for i in range(len(vectors)):\n",
    "    vec[i] = vectors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119678, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь соединим статистическую и семантическую части наших текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((vecs, vec), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vec, vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализуем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Масштабирование проведено корректно, scaler следует настраивать (fit) только по обучающей выборке.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del text_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec1 = vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec1 = tsvd.transform(test_vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [text2vec(x) for x in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec2 = np.zeros((len(vectors), 100))\n",
    "for i in range(len(vectors)):\n",
    "    test_vec2[i] = vectors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate((test_vec1, test_vec2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39893, 200)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_vec1, test_vec2, text_test, w2v, tsvd, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы выбрать лучшие гиперпараметры для модели проведем поиск по сетке с кроссвалидацией с помощью StratifiedKFold, т.к. мы имеем неодинаковые доли классов 0 и 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(3, shuffle=True)\n",
    "scorer = make_scorer(F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Отлично, получается размер валидационной выборки при проведении кросс-валидации равен 75% * 1/3 = 25% от исходной выборки, то есть совпадает с размером тестовой выборки.\n",
    "    \n",
    "В принципе, размеры тестовой и валидационной выборок можно было уменьшить (но размер каждой должен составлять не менее 10% от исходной выборки), чтобы было больше данных для обучения.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=SGDClassifier(alpha=0.0001, average=False,\n",
       "                                     class_weight=None, early_stopping=False,\n",
       "                                     epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "                                     l1_ratio=0.15, learning_rate='optimal',\n",
       "                                     loss='hinge', max_iter=1000,\n",
       "                                     n_iter_no_change=5, n_jobs=None,\n",
       "                                     penalty='l2', power_t=0.5,\n",
       "                                     random_s...e, tol=0.001,\n",
       "                                     validation_fraction=0.1, verbose=0,\n",
       "                                     warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'loss': ['hinge', 'log', 'modified_huber',\n",
       "                                  'squared_hinge', 'perceptron', 'squared_loss',\n",
       "                                  'huber', 'epsilon_insensitive',\n",
       "                                  'squared_epsilon_insensitive']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber',\n",
    "    'squared_hinge', 'perceptron', 'squared_loss',\n",
    "    'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "}\n",
    "sgd_grid = GridSearchCV(SGDClassifier(), params, cv=cv, scoring=scorer)\n",
    "sgd_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Можно было задать scoring='f1'.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7352048895927658, {'loss': 'log'})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_grid.best_score_, sgd_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат - 0.735 дала стохастическая оптимизация логистической регрессии (logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Стоило вывести на экран лучшую модель sgd_grid.best_params_.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sgd_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7576325656132833"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Побили бейслайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем поработать с алгоритмом градиентного бустинга из библиотеки lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "                                      colsample_bytree=1.0,\n",
       "                                      importance_type='split',\n",
       "                                      learning_rate=0.1, max_depth=-1,\n",
       "                                      min_child_samples=20,\n",
       "                                      min_child_weight=0.001,\n",
       "                                      min_split_gain=0.0, n_estimators=15,\n",
       "                                      n_jobs=-1, num_leaves=31, objective=None,\n",
       "                                      random_state=None, reg_alpha=0.0,\n",
       "                                      reg_lambda=0.0, silent=True,\n",
       "                                      subsample=1.0, subsample_for_bin=200000,\n",
       "                                      subsample_freq=0),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'boosting_type': ['gbdt', 'dart', 'goss'],\n",
       "                         'max_depth': [3, 5, 7, -1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': ['gbdt', 'dart', 'goss'],\n",
    "    'max_depth': [3, 5, 7, -1],\n",
    "    \n",
    "}\n",
    "gb_grid = GridSearchCV(LGBMClassifier(n_estimators=15), params, cv=cv, scoring=scorer)\n",
    "gb_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Молодец, что настраиваешь гиперпараметры моделей.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6526128365935363, {'boosting_type': 'gbdt', 'max_depth': -1})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_grid.best_score_, gb_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем лучшие параметры и увеличим количество деревьев решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7451635351426583\n",
      "0.7541766109785203\n",
      "0.7435538116591928\n"
     ]
    }
   ],
   "source": [
    "sc = []\n",
    "for train, test in cv.split(X_train, y_train):\n",
    "    X_tr, X_v, y_tr, y_v = X_train[train], X_train[test], y_train[train], y_train[test]\n",
    "    gb_clf = LGBMClassifier(**gb_grid.best_params_, n_estimators=200)\n",
    "    gb_clf.fit(X_tr, y_tr)\n",
    "    score = F1(y_v, gb_clf.predict(X_v))\n",
    "    sc.append(score)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7476313192601238"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sc).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили результат на кроссвалидации для градиентного бустинга лучше, чем для \"логистической регрессии\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=200, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf = LGBMClassifier(**gb_grid.best_params_, n_estimators=200)\n",
    "gb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7524088814411395"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим теперь на обычную логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01]),\n",
       "                         'max_iter': [75, 150, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'C': np.logspace(-3, 1, 5, base=10),\n",
    "    'max_iter': [75, 150, 500]\n",
    "}\n",
    "logreg_grid = GridSearchCV(LogisticRegression(), params, cv=cv, scoring=scorer)\n",
    "logreg_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7476822480603601, {'C': 10.0, 'max_iter': 75})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_grid.best_score_, logreg_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logreg_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7608238755779739"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получили следующие результаты на кроссвалидации и тестовой выборке:\n",
    "- Логистическая регрессия - 0.7476822480603601, 0.7608238755779739\n",
    "- Стохастическая оптимизация линейных моделей (логистической регрессии) - 0.7352048895927658, 0.7576325656132833\n",
    "- Градиентный бустинг на 200 деревьях решений - 0.7476313192601238, 0.7524088814411395\n",
    "\n",
    "Все решения побили бейслайн, но лучшей по всем параметрам оказалась логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Стоило рассмотреть еще модель логистической регрессии.\n",
    "    \n",
    "Строго говоря, выбирать лучшую модель по качеству на **тестовой** выборке некорректно, так как тогда значение качества получается оптимистически смещенным. В целом, возможны 2 способа настройки и выбора моделей: \n",
    "\n",
    "1) делим выборку на 2 части – обучающую и тестовую. Для всех моделей на обучающей выборке проводим кросс-валидацию, лучшую модель выбираем по качеству на кросс-валидации. В конце оцениваем качество лучшей модели на тестовой выборке. \n",
    "    \n",
    "2) делим выборку на 3 части – обучающую, валидационную, тестовую. На обучающей выборке настраиваем модели, лучшую модель выбираем по качеству на валидационной выборке. Качество лучшей модели оцениваем на тестовой выборке.\n",
    "\n",
    "Твои эксперименты относятся к первому способу, поэтому по правилам мы должны были бы выбрать SGDClassifier в качестве итоговой модели (судя по качеству на кросс-валидации).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я научился работать с текстовыми данными, очищать их, приводить в понятный алгоритмам вид, используя статистические и семантические признаки текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Итоговый комментарий:</b> \n",
    "\n",
    "Спасибо, ты провел очень качественное исследование, осталось его немного доработать.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
